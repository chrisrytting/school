\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\subsection*{Exercise 4.1}
Since we know that for some $k \in \mathbb{R}$ $A^{k} = 0$, and since $A \in M_{n}(\mathbb{F})$ we know that $(\lambda I - A)x = 0$ for some non-zero $x$. Let $k$ be the smallest $k$ such that $A^{k} = 0$. With these properties in mind we show that:

\begin{align*}
&(\lambda I - A)x = 0\\
&(A \lambda I - A^{2})x = 0\\
&~~~~~~~\vdots\\
&(A^{k-1} \lambda I - A^{k})x = 0\\
&\lambda(A^{k-1})x = 0\\
&\lambda(AA...Ax) = 0\\
&\lambda^{k}x = 0\\
&\lambda = 0
\end{align*}

\subsection*{Exercise 4.3}

Express any matrix $A \in M_n(\mathbb{F})$ such that

\[ A = \begin{bmatrix}
    a & b \\
    c & d
    \end{bmatrix} \]

Then we have that the characteristic polynomial 

\[p(\lambda) = (a-\lambda)(d-\lambda) - bc\]
\[p(\lambda) = ad - \lambda d - \lambda a + \lambda^2 - bc\]
\[p(\lambda) = \lambda^2 - \lambda (d + a)+ ad - bc \]
\[p(\lambda) = \lambda^2 - \lambda ~\textrm{tr}(A) + det(A) \]

\subsection*{Exercise 4.7}
From Exercise 2 we know that\\
\smallskip\\
$D = \begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0
\end{bmatrix}$\\
\smallskip\\
Thus the Charcteristic equation is given by
\[ \begin{vmatrix}
-\lambda & 1 & 0\\
0 & -\lambda & 2\\
0 & 0 & -\lambda
\end{vmatrix} \Rightarrow \lambda^3 = 0 \Rightarrow \lambda = 0\]
The eigen vectors are then given by the nullspace of \\
\smallskip\\ 
$\begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0
\end{bmatrix}$  
$\Rightarrow$ 
$\begin{bmatrix} 
    -1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 2
\end{bmatrix}$
$\Rightarrow$
$\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}$\\
\smallskip\\
Because the number of eigenvectors is less than the rank of D, we know that 
it is not semi-simple.

\subsection*{Exercise 4.9}
Since $A$ is semisimple we know that $A = PDP^{-1}$, and that $A$ is similar to $D$. 

\begin{align*}
F(A) &= a^{0}A^{0} + a^{1}A + ... + a^{n-1}A^{n-1} + a^{n}A^{n} \\ \\
F(PDP^{-1}) &= a^{0}(PDP^{-1})^{0} + a^{1}(PDP^{-1}) + ... + a^{n-1}(PDP^{-1})^{n-1} + a^{n}(PDP^{-1})^{n}\\
&= P[a^{0}D^{0} + a^{1}D + ... + a^{n-1}D^{n-1} + a^{n}D^{n}]P^{-1}\\ \\
a_{n}D^{n} &= \{a_{n}\lambda_{1}^{n}, a_{n}\lambda_{2}^{n}, ...\} \\ \\
F(A) &= PF(D)P^{-1}
\end{align*}
By proposition 4.2.9 we know $F(A)$ and $F(D)$ have the same eigenvalues, and since $F(D) = \{F(\lambda_{i})\}$ we can say that the eigenvalues of $F(A)$ are $\{F(\lambda_{i})\}$.

\subsection*{Exercise 4.11 (i)}

Note that \\
\[ \ell A = \lambda \ell \]
Therefore,
\[ \ell A r = \lambda \ell r \]

\[ \ell (A r) = \lambda \ell r \]

\[ \ell \rho r = \lambda \ell r \]

\[ \rho \ell r = \lambda \ell r \]
 
Since $\rho \neq \lambda$, the only way for this equality to hold is if $\ell r = 0$.



\subsection*{Exercise 4.11 (ii)}

By Remark 4.2.14, if a matrix is semisimple, it has a basis of right eigenvectors $\{\mathbf{p_1,...,p_n}\}$, and those right eigenvectors form the columns of a matrix $P$ which diagonalizes $A$. Furthermore, for each $i$ the $i$th row $\mathbf{q_i}$ of $P^{-1}$ is a left eigenvector of $A$ with eigenvalue $d_i$. Therefore, if we diagonalize the matrix $A$ and multiply $P^{-1}$ by $P$, we get the $I$, and we see that the rows $\mathbf{q_i}$ of $P^{-1}$ multiplied by the columns $\mathbf{p_i}$ of $P$ equals one at every entry of the identity's diagonal.

\[P^{-1}     P = I \]

\subsection*{Exercise 4.11 (iii)}

Consider the matrix 

\[A = \begin{bmatrix}
1 & -1\\
1 & 3
\end{bmatrix}\]

\begin{align*}
\lambda &= \rho = 2 \neq 0\\
\mathbf{\ell} &= \begin{bmatrix}
1\\
1
\end{bmatrix} \ \neq 0\\
\mathbf{r} &= \begin{bmatrix}
-1\\
1
\end{bmatrix} \neq 0 \\
\mathbf{\ell r} &= 0
\end{align*}

Note that the eigenvectors are distinct and non-zero, while the eigenvalues are the same and non-zero. Their product, however, is zero.

\subsection*{Exercise 4.13}

Since $T$ is an orthonormal operator on $V$, we know that there is some matrix $D$ and a matrix $P$ such that 
\begin{align*}
T = PDP^{-1}
\end{align*}
Now, let 
\begin{align*}
Q = PV^{-1}
\end{align*}
By theorem, since $P$ and $V^{-1}$ are orthogonal, their product, $Q$, is orthogonal. Furthermore, $P$ is an orthonormal eigenbasis for $T$. Now we have that
\begin{align*}
Q &= PV^{-1}\\
QV &= PV^{-1}V\\
QV &= P
\end{align*}
Therefore, $QV^{-1}$ is at least an orthogonal eigenbasis for $T$, and since we are trying to prove the existence of $Q$, we can divide each column of the eigenbasis by its length in order to normalize the eigenbasis, and $QV^{-1}$ is an orthonormal eigenbasis for $T$.

\subsection*{Exercise 4.15}
\begin{enumerate}
\item
By proposition 3.7.12, If $V$ is a finite-dimensional inner product space, then we have
\begin{align*}
&(i). If ~S \in V,~ then~ S^{**} = S\\
&(ii). If ~S,T \in V, ~then (ST)^* = T^*S^*
\end{align*}
Therefore we have that 
\begin{align*}
(TT^*)^* = T^{**}T^* = TT^* \\
\end{align*}
So $TT^*$ is self-adjoint. We also have that
\begin{align*}
\langle \mathbf{w}, TT^* \mathbf{w} \rangle = \langle T^* \mathbf{w}, T^* \mathbf{w} \rangle \geq 0
\end{align*}
\item Because $TT^*$ is normal, it is orthonoramlly similar to a diagonal matrix $D$. Thus, 
\[ TT^* = UDU^{-1} \]
Thus, define $S = U\sqrt{D}U^{-1}$, where $\sqrt{D}$ is the elementwise square root of $D$.\\
\[ S^2 = U\sqrt{D}U^{-1}U\sqrt{D}U^{-1} = U\sqrt{U}^2U{-1} = UDU^{-1} = TT^*\]
To show that it is self-adjoint, Note 
\begin{align*}
 SS &= TT^*\\
 S &= TT^*S^{-1}\\
 S^* &= (TT^*S^{-1})^*\\
 S^* &= S^{-1*}TT^*\\
 S^*S^* &= TT^* = SS\\
 \end{align*}
 Which implies that $S=S^*$
\item It is sufficent to show that $(S^{-1}T)^*S^{-1}T = I$
\begin{align*} 
(S^{-1}T)^*S^{-1}T &= T^*(S^{-1})^*S^{-1}T\\
&= T^*S^{-1}S^{-1}T\\
&= T^*(S^2)^{-1}T\\
&= T^*(TT^*)^{-1}T\\
&= T(T^*)^{-1}T^{-1}T\\
&= I\\
\end{align*}
\end {enumerate}

\subsection*{Exercise 4.17}
i) \\ \\
$A$ is normal with eigenvalues $\{\lambda_{1},\lambda_{2},...,\lambda_{n}\}$ and corresponding orthonormal eigenvectors$\{x_{1}, x_{2},..., x_{n}\}$. We can use the decomposition $A = UDU^{-1}$. $U$ is the orthonormal matrix of the eigenvectors and therefore $U^{H}=U^{-1}$ and that$U^{H}U = UU^{H}$. We know that $UU^{-1} = UU^{H} = U^{H}U$ so $U^{H}U = I$. Showing that $U^{H}U = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$ will prove that $I = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$. \\ \\
Since the vectors of $U$ are orthonormal when we multiple any of the columns of $U$ against any of the rows of $U^{H}$ which do not correspond to the same eigenvector this results in a zero norm, and when they do correspond this returns $x_{i}x_{i}^{H}$, showing that $U^{H}U = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$, thus proving that $I = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$. \\ 

ii) Using the same decomposition as in (i), we can express $A$ in the following manner:
\begin{align*}A &= U^HDU\\
& = \begin{bmatrix}
\mathbf{x_1, x_2, ..., x_n}\end{bmatrix}^{\mathbf{H}}
\begin{bmatrix}
{\lambda_1, \lambda_2, ..., \lambda_n}
\end{bmatrix}
\begin{bmatrix}
\mathbf{x_1, x_2, ..., x_n}\end{bmatrix}
\end{align*}
where the columns of $U$ are the columns $\{\mathbf{x_1, x_2, ..., x_n}\}$, the diagonal of $D$ consists of the eigenvalues $\{\lambda_1, \lambda_2, ..., \lambda_n\}$, and the rows of $U^H$ consist of $\{\mathbf{x_1^H, x_2^H, ..., x_n^H}\}$. If we multiply these matrices, we get 
\begin{align*} A &= \begin{bmatrix}
\lambda_1\mathbf{\bar{x}_{11}x_{11}} + \lambda_2\mathbf{\bar{x}_{21}x_{21}} +...+ \lambda_n\mathbf{\bar{x}_{n1}x_{n1}} & ... & 
\lambda_1\mathbf{\bar{x}_{1n}x_{11}} + \lambda_2\mathbf{\bar{x}_{2n}x_{21}} +...+\lambda_n\mathbf{\bar{x}_{nn}x_{n1}} \\
\vdots&\ddots&\vdots\\
\lambda_1\mathbf{\bar{x}_{11}x_{1n}} + \lambda_2\mathbf{\bar{x}_{21}x_{2n}} +...+\lambda_n\mathbf{\bar{x}_{n1}x_{nn}} & ... &
\lambda_1\mathbf{\bar{x}_{1n}x_{1n}} + \lambda_2\mathbf{\bar{x}_{2n}x_{2n}} +...+ \lambda_n\mathbf{\bar{x}_{nn}x_{nn}} 
\end{bmatrix}\\
\\
&= \lambda_1 \mathbf{x_1x_1^H} + \lambda_2 \mathbf{x_2x_2^H} + ... + \lambda_n \mathbf{x_nx_n^H}
\end{align*}

\subsection*{Exercise 4.19}
\[ P = \begin{bmatrix} 
I & -B^{-1}C\\
0 & I
\end{bmatrix}
\]
\[ P^HAP = \begin{bmatrix}
B & 0\\
0 & D - C^HB^{-1}C
\end{bmatrix} = F
\]\\
Note that F is orthonormally similar to A, thus if F is Positive Definite, A is, because they both have the same Eigenvalues. Also, 
%\[ F = \begin{} \] 

\subsection*{Exercise 4.23}

part iii)\\ \\
In the first part we proved that $\|A\|_{2} = \sigma_{1}$ and it immediately follows that $\|A\|_{2}^{2} = \sigma_{1}^{2}$ \\ \\
We now want to show that $\|A^{H}\|_{2}^{2} = \sigma_{1}^{2}$. Using the SVD we can show that:\\
   \[ A^{H} = (U \Sigma V^{H})^{H} = V \Sigma^{H} U^{H} \]
   Since we know that $U$ and $V$ are orthonormal matrices, we know that $U^{H}$ and $V$ are also orthonormal with the singular values of $\Sigma$ being the same as $\Sigma^{H}$ thus $\|A^{H}\|_{2}^{2} = \sigma_{1}^{2}$ \\ \\
   Next we want to show that $\|A^{T}\|_{2}^{2} = \sigma_{1}^{2}$. This follows by a similar procedure. Using SVD we can show that: \\
   \[ A^{T} = (U \Sigma V^{H})^{T} = \bar{V} \Sigma^{T} U^{T} \]
   By similar reasoning as above we know that $\|A^{T}\|_{2}^{2} = \sigma_{1}^{2}$. \\ \\
   Lastly, we want to show that $\|A^{H}A\|_{2} = \sigma_{1}^{2}$. Using the SVD we can show that: \\
   \[ A^{H}A = (U\Sigma V^{H})^{H}(U\Sigma V^{H}) = V\Sigma^{H}U^{H} U\Sigma V^{H} = V\Sigma^{H}\Sigma V^{H} \]
   Since we know that $V$ and consequently $V^{H}$ are orhtonormal we have our SVD with our $\Sigma_{new} = \Sigma^{H}\Sigma$ whose diagonals are the same as $\sigma_{n}^{2}$. This means that our maximum value will be $\sigma_{1}^{2}$ showing that $\|A^{H}A\|_{2} = \sigma_{1}^{2}$.


\end{document}
