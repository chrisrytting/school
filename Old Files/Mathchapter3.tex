
\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\subsection*{4.1}
We know that for some $k \in \mathbb{R}$, $A^k = 0$, and because A is square, we know that $(\lambda I - A)x = 0$ for some non-zero $x$. Let $k$ be the smallest $k$ such that $A^k = 0$. Then we have that
\begin{align*}
    (\lambda I - A)x &= 0\\
    (A\lambda I - A^2)x &= 0\\
    (A^2\lambda I - A^3)x &= 0\\
    \vdots\\
    (A^{k-1}\lambda I - A^k)  x &= 0\\
    \lambda (A^{k-1}) x &= 0\\
    \lambda AAA \dots Ax &= 0\\
    \lambda^k x &= 0\\
\end{align*}
Since $x$ is non-zero, we know that $\lambda=0$.

\subsection*{4.3}

Express $A$ such that 

\[A =
\begin{bmatrix}
    a & b\\
    c & d
\end{bmatrix}\]
Then we have that the characteristic polynomial is as follows.
\[p(\lambda) = (a-\lambda)(d-\lambda) - bc\]
\[p(\lambda) = ad - \lambda d - \lambda a + \lambda^2 - bc\]
\[p(\lambda) = \lambda^2 - \lambda (d + a)+ ad - bc \]
\[p(\lambda) = \lambda^2 - \lambda ~\textrm{tr}(A) + det(A) \]


\subsection*{4.7}

We know from exercise 2 that

\begin{align*}
    D &= 
\begin{bmatrix}
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0\\
\end{bmatrix}
\end{align*}

The nullspace is then given by 
The characteristic equation, then is given by 
\begin{align*}
\lambda^3 &= 0\\
\implies \lambda &= 0
\end{align*}
$\begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 1\\
    0 & 0 & 0
\end{bmatrix}$  
$\Rightarrow$ 
$\begin{bmatrix} 
    -1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1 
\end{bmatrix}$
$\Rightarrow$
$\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}$\\

\bigskip

Since columns in the null space are less than the rank, $D$ is not semi-simple.
\subsection*{4.9}
$A$ is semisimple, so we can express $A$ as $A = PDP^{-1}$, $A$ being similar to $D$. 

\begin{align*}
F(A) &= a^{0}A^{0} + a^{1}A + ... + a^{n-1}A^{n-1} + a^{n}A^{n} \\ \\
F(PDP^{-1}) &= a^{0}(PDP^{-1})^{0} + a^{1}(PDP^{-1}) + ... + a^{n-1}(PDP^{-1})^{n-1} + a^{n}(PDP^{-1})^{n}\\
&= P[a^{0}D^{0} + a^{1}D + ... + a^{n-1}D^{n-1} + a^{n}D^{n}]P^{-1}\\ \\
a_{n}D^{n} &= \{a_{n}\lambda_{1}^{n}, a_{n}\lambda_{2}^{n}, ...\} \\ \\
F(A) &= PF(D)P^{-1}
\end{align*}
We know, by proposition 4.2.9, that $F(A)$ and $F(D)$ identical eigenvalues. Since $F(D) = \{F(\lambda_{i})\}$ the eigenvalues of $F(A)$ are $\{F(\lambda_{i})\}$.


\subsection*{Exercise 4.11 (i)}

Note that \\
\[  \mathbf{\ell} A = \lambda \mathbf{\ell} \]
Therefore,
\[ \mathbf{\ell}A \mathbf{r} = \lambda \mathbf{\ell} \mathbf{r} \]

\[ \mathbf{\ell}(A \mathbf{r}) = \lambda \mathbf{\ell}\mathbf{r} \]

\[ \mathbf{\ell}\rho \mathbf{r} = \lambda \mathbf{\ell}\mathbf{r} \]

\[ \rho \mathbf{\ell}\mathbf{r} = \lambda \mathbf{\ell} \mathbf{r}\]
 
Since $\rho \neq \lambda$, the only way for this equality to hold is if $ \mathbf{\ell}\mathbf{r}= 0$.

\subsection*{4.11 (ii)}
By Remark 4.2.14, if a matrix is semisimple, it has a basis of right eigenvectors $\{\mathbf{r_1,...,r_n}\}$, and those right eigenvectors form the columns of a matrix $P$ which diagonalizes $A$. It also has a basis of left eigenvectors $\{\mathbf{\ell_1,...,\ell_n}\}$ that make up the rows of $P{^-1}$. So we have that $A = PDP^{-1}$, and that $P^{-1}P = I$. Since the product $P^{-1}P$ is made up solely of products of some $l_i$ and some $\mathbf{r_i}$, and is the identity, there necessarily must be some $l_i$ and some $\mathbf{r_i}$ that yield a 1.

\subsection*{4.11 (iii)}

Consider the matrix 

\[A = \begin{bmatrix}
1 & -1\\
1 & 3
\end{bmatrix}\]

\begin{align*}
\lambda &= \rho = 2 \neq 0\\
\mathbf{\ell} &= \begin{bmatrix}
1\\
1
\end{bmatrix} \ \neq 0\\
\mathbf{r} &= \begin{bmatrix}
-1\\
1
\end{bmatrix} \neq 0 \\
\mathbf{\ell r} &= 0
\end{align*}

Note that the eigenvectors are distinct and non-zero, while the eigenvalues are the same and non-zero. Their product, however, is zero.


\subsection*{4.13}

Since $T$ is an orthonormal operator on $V$, we know that there is some matrix $D$ and a matrix $P$ such that 
\begin{align*}
T = PDP^{-1}
\end{align*}
Now, let 
\begin{align*}
Q = PV^{-1}
\end{align*}
By theorem, since $P$ and $V^{-1}$ are orthogonal, their product, $Q$, is orthogonal. Furthermore, $P$ is an orthonormal eigenbasis for $T$. Now we have that
\begin{align*}
Q &= PV^{-1}\\
QV &= PV^{-1}V\\
QV &= P
\end{align*}
Therefore, $QV^{-1}$ is at least an orthogonal eigenbasis for $T$, and since we are trying to prove the existence of $Q$, we can divide each column of the eigenbasis by its length in order to normalize the eigenbasis, and $QV^{-1}$ is an orthonormal eigenbasis for $T$.


\subsection*{4.15 (i)}
By proposition 3.7.12, If $V$ is a finite-dimensional inner product space, then we have
\begin{align*}
&(i). If ~S \in V,~ then~ S^{**} = S\\
&(ii). If ~S,T \in V, ~then (ST)^* = T^*S^*
\end{align*}
Therefore we have that 
\begin{align*}
(TT^*)^* = T^{**}T^* = TT^* \\
\end{align*}
So $TT^*$ is self-adjoint. We also have that
\begin{align*}
\langle \mathbf{w}, TT^* \mathbf{w} \rangle = \langle T^* \mathbf{w}, T^* \mathbf{w} \rangle \geq 0
\end{align*}


\subsection*{4.15 (ii)}
As $TT^*$ is normal, we have that $TT^* = UDU^{-1}$. Let $T = USU^{-1}$ where $S$ is the matrix $D$'s element-wise square root. Note that
\begin{align*}
S^2 = U\sqrt{D}
    
\end{align*}


\subsection*{4.19}
$(\Rightarrow)$ Consider the following:
\begin{align}
A &= \begin{bmatrix}
B & C \\ 
C^H & D
\end{bmatrix}\\
P^HAP &= \begin{bmatrix}
B & BE + C \\
E^HB +C^H & E^HBE + C^HE + E^HC + D
\end{bmatrix}\\
&=\begin{bmatrix}
B & 0\\
0 & D - C^HB^{-1}C
\end{bmatrix}\\
&= \begin{bmatrix}
B&0\\
0 &0
\end{bmatrix}
+
\begin{bmatrix}
0&0\\
0 & D-C^HB^{-1}C
\end{bmatrix}
\end{align}
Since both matrices of (4) are positive definite, meaning that they have strictly positive eigenvalues. Therefore, their sum, call it matrix $B$, will be positive definite, and since $B$ is similar to $A$, $A$ is also positive definite.\\
$(\Leftarrow)$ Taking into account the facts from the forward part of the proof, we now have that $A$ is positive definite, so $B$ is positive definite, and the two matrices that make it up in (4) are therefore positive definite.

\subsection*{4.23 (iv)}
Since $A$ is an $mxn$ matrix, we can express it as 
\begin{align*}
    A = W\Sigma X^T
\end{align*}
where $W$ and $X$ are orthonormal matrices. Now, we have
\begin{align*}
    \|UAV\|_2 &= \|A\|_2\\
    &= \|UW\Sigma X^TV\|_2 
\end{align*}
Since we know the product of two orthonormal matrices is orthonormal, we know that the product of $U$ and $W$, call it $B$, and the product of $X^T$ and $V$, call it $C$, is orthonormal. Therefore, $A$ fulfills the criteria of the SVD and since we've already found that $\|A\|_2 = \sigma_1$ in exercise (i), $UAV$ has a singular decomposition with the same $\Sigma$ matrix as $A$, so $\|UAV\|_2 = \sigma_1$.

\subsection*{4.27}
Consider the matrix
\begin{align*}
    A = \begin{bmatrix}
    -1 & 0\\
    0 & -3
    \end{bmatrix}
\end{align*}
which has negative eigenvalues $-1$ and $-3$. Its determinant is non-zero and yet needs positive, real singular values. Therefore, the eigenvalues differ from the singular values and thus fulfills the criteria specified by the exercise.










\end{document}
