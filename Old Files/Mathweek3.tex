
\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\subsection*{4.1}
We know that for some $k \in \mathbb{R}$, $A^k = 0$, and because A is square, we know that $(\lambda I - A)x = 0$ for some non-zero $x$. Let $k$ be the smallest $k$ such that $A^k = 0$. Then we have that
\begin{align*}
    (\lambda I - A)x &= 0\\
    (A\lambda I - A^2)x &= 0\\
    (A^2\lambda I - A^3)x &= 0\\
    \vdots\\
    (A^{k-1}\lambda I - A^k)  x &= 0\\
    \lambda (A^{k-1}) x &= 0\\
    \lambda AAA \dots Ax &= 0\\
    \lambda^k x &= 0\\
\end{align*}
Since $x$ is non-zero, we know that $\lambda=0$.
\subsection*{4.2}

\begin{align*}
    D &= 
\begin{bmatrix}
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0\\
\end{bmatrix}
\end{align*}

The only eigenvalue of D is $0$, which has an algebraic multiplicity of 3. The eigenvalue that corresponds is 
\[
\begin{bmatrix}

    1\\
    0\\
    0
    
\end{bmatrix}
\]
whose geometric multiplicity is 1.
\subsection*{4.3}

Express $A$ such that 

\[A =
\begin{bmatrix}
    a & b\\
    c & d
\end{bmatrix}\]
Then we have that the characteristic polynomial is as follows.
\[p(\lambda) = (a-\lambda)(d-\lambda) - bc\]
\[p(\lambda) = ad - \lambda d - \lambda a + \lambda^2 - bc\]
\[p(\lambda) = \lambda^2 - \lambda (d + a)+ ad - bc \]
\[p(\lambda) = \lambda^2 - \lambda ~\textrm{tr}(A) + det(A) \]

\subsection*{4.4 (i)}
Let 
\[A =
\begin{bmatrix}
   a & b \\ 
   c & d
\end{bmatrix}  \]

Since A is Hermitian, $A = A^H$, we know that $c=b$, and if we take the characteristic polynomial, we find that $
 \lambda = \frac{(a+d) \pm \sqrt{(a+d)^2 - 4(1)(ad-b^2)    }   }{    2} = \frac{a+d}{2}+\frac{1}{2}\sqrt{a^2 + 2ad + d^2 -4ad + 4b^2} = \frac{a+d}{2}+\frac{1}{2}\sqrt{a^2 - 2ad + d^2 + 4b^2 } = \frac{a+d}{2}+\frac{1}{2}\sqrt{(a-d)^2 + 4b^2  }$.\\
 Since all elements of $A$ are real, and $(a-d)^2 + 4b^2 $ will be non-negative, we know that no eigenvalue will be imaginary, implying that all will be real. 

\subsection*{4.4 (ii)}

Because $A$ is skew-Hermitian, we known that $-a_{11} = \bar{a_{11}}$ and that  $-a_{22} = \bar{a_{22}}$, suggesting that A has solely imaginary numbers along the diagonal. Note also that  $-a_{12} = \bar{a_{21}}$ and that  $-a_{21} = \bar{a_{12}}$. By finding the conjugate of $a_{12}$ and $a_{21}$, we can see that $a_{12} = -x + iy$ and that $a_{21} = x + iy$ and that $a_{11} = wi$ and $a_{11} = vi$. Let
\[ A = 
\begin{bmatrix}
    wi & -x + iy \\
    x + iy & vi
\end{bmatrix}\]
With $x,y,v,w \in \mathbb{R}$\\
Upon finding the characteristic polynomial, we find that 
\[ 
\lambda = i\left(\frac{1}{2}(w + v) \pm \frac{1}{2}\sqrt{ (w-v)^2 +4x^2  +4y^2    }\right)
\] 
Since $(w-v)^2 +4x^2  +4y^2$ is real and non-negative and ${w+v} \over 2$ is real, the sum of these will be real, and a real number multiplied by an imaginary number will be purely imaginary. 
\subsection*{4.6}
\[P = 
\begin{bmatrix}
    1 \over \sqrt{2}  &  1 \over \sqrt{5}\\
     -1 \over \sqrt{2} &  1 \over \sqrt{5}
\end{bmatrix}
\]


\subsection*{4.7}

We know from exercise 2 that

\begin{align*}
    D &= 
\begin{bmatrix}
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0\\
\end{bmatrix}
\end{align*}

The nullspace is then given by 
The characteristic equation, then is given by 
\begin{align*}
\lambda^3 &= 0\\
\implies \lambda &= 0
\end{align*}
$\begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 1\\
    0 & 0 & 0
\end{bmatrix}$  
$\Rightarrow$ 
$\begin{bmatrix} 
    -1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 1 
\end{bmatrix}$
$\Rightarrow$
$\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}$\\

\bigskip

Since columns in the null space are less than the rank, $D$ is not semi-simple.

\subsection*{4.8 (i)}
\[B = P
\begin{bmatrix}
    0 & 0\\
    0 & 1
    
\end{bmatrix}
P^{-1}\]
\[A^n - B = {1\over3}({2\over5})^n
\begin{bmatrix}
    1 & -2 \\
    -1 & 2
    
\end{bmatrix}\]
\[\|{ A^n - B}\|_1 = 2 \left| ({2 \over 5})^n \right| \]
Which converges, so for every $\varepsilon$ we can choose an $N \in \mathbb{N}$ such that $\| A^n - B\|_1 < \varepsilon$

As for the infinity norm, \[ \|{ A^n - B}\|_{\infty} = |\frac{2}{3}(\frac{2}{5})^n | \]. The same logic applies here as for the 1-norm since this series also converges. \\ 

As for the 2-norm, we have 
\[  \|{ A^n - B}\|_2 = \sqrt{10} |{1\over3}({2\over5})^n(1)| \]
The same logic applies here as for the 1-norm and the infinity-norm.

As for the Frobenius norm, we have

 $||A^n-B||_{F} = || 
  \frac{1}{3}(\frac{2}{5})^n
  \begin{bmatrix}
   1 &  -2 \\
   -1 &  2\\
  \end{bmatrix}    ||_{F} $\\
  \[= \sqrt{ tr\left(\left(\frac{1}{3}(\frac{2}{5})^n
  \begin{bmatrix}
   1 &  -2 \\
   -1 &  2\\
  \end{bmatrix}\right)\left( \frac{1}{3}(\frac{2}{5})^n
  \begin{bmatrix}
   1 &  -2 \\
   -1 &  2\\
  \end{bmatrix}  \right)^H  \right)}\]
  \[ = \sqrt{ tr\left(\frac{1}{3}(\frac{2}{5})^n\frac{1}{3}(\frac{2}{5})^n\begin{bmatrix}
   1 &  -2 \\
   -1 &  2\\
  \end{bmatrix}\begin{bmatrix}
   1 &  -1 \\
   -2 &  2\\
  \end{bmatrix}      \right) }\]
  
  \[ = \sqrt{\frac{1}{9}\left( \frac{2}{5}  \right)^{2n}tr\left(\begin{bmatrix}
   5 &  -5 \\
   -5 &  5\\
  \end{bmatrix}\right) } = \frac{1}{3}\left(\frac{2}{5}\right)^n\sqrt{10} \]
  
  This converges and the same logic applies as for the other norms. Some norms converge more quickly than others, but they all converge.


\subsection*{4.8 (ii)}
The eigenvalues are $\lambda_1 = 8.44$ and $\lambda_2 = 5.28 $\\

\subsection*{4.9}
$A$ is semisimple, so we can express $A$ as $A = PDP^{-1}$, $A$ being similar to $D$. 

\begin{align*}
F(A) &= a^{0}A^{0} + a^{1}A + ... + a^{n-1}A^{n-1} + a^{n}A^{n} \\ \\
F(PDP^{-1}) &= a^{0}(PDP^{-1})^{0} + a^{1}(PDP^{-1}) + ... + a^{n-1}(PDP^{-1})^{n-1} + a^{n}(PDP^{-1})^{n}\\
&= P[a^{0}D^{0} + a^{1}D + ... + a^{n-1}D^{n-1} + a^{n}D^{n}]P^{-1}\\ \\
a_{n}D^{n} &= \{a_{n}\lambda_{1}^{n}, a_{n}\lambda_{2}^{n}, ...\} \\ \\
F(A) &= PF(D)P^{-1}
\end{align*}
We know, by proposition 4.2.9, that $F(A)$ and $F(D)$ identical eigenvalues. Since $F(D) = \{F(\lambda_{i})\}$ the eigenvalues of $F(A)$ are $\{F(\lambda_{i})\}$.

\subsection*{4.10}

Since A is semi-simple, express it as
\[A = PDP^{-1}\]
and we have that
\[
p(A) = a_1PDP^{-1} + \dots + a_n PDP^{-1}\\
= P(a_1 D +\dots+ a_n D) P^{-1}\\
= PCP^{-1} \]
where C is a matrix with the characteristic polynomial $P(\lambda_i)$ on the diagonal. We know, though, that $P(\lambda_i)$ is equal to zero for all $i$ by the definition of the characteristic polynomial, so we have 
\[p(A) = PZP^{-1}\]
where $Z$ is a matrix of zeros, so we see that $p(A) = 0$.



\subsection*{4.11 (i)}

Note that \\
\[  \mathbf{\ell} A = \lambda \mathbf{\ell} \]
Therefore,
\[ \mathbf{\ell}A \mathbf{r} = \lambda \mathbf{\ell} \mathbf{r} \]

\[ \mathbf{\ell}(A \mathbf{r}) = \lambda \mathbf{\ell}\mathbf{r} \]

\[ \mathbf{\ell}\rho \mathbf{r} = \lambda \mathbf{\ell}\mathbf{r} \]

\[ \rho \mathbf{\ell}\mathbf{r} = \lambda \mathbf{\ell} \mathbf{r}\]
 
Since $\rho \neq \lambda$, the only way for this equality to hold is if $ \mathbf{\ell}\mathbf{r}= 0$.

\subsection*{4.11 (ii)}
By Remark 4.2.14, if a matrix is semisimple, it has a basis of right eigenvectors $\{\mathbf{r_1,...,r_n}\}$, and those right eigenvectors form the columns of a matrix $P$ which diagonalizes $A$. It also has a basis of left eigenvectors $\{\mathbf{\ell_1,...,\ell_n}\}$ that make up the rows of $P{^-1}$. So we have that $A = PDP^{-1}$, and that $P^{-1}P = I$. Since the product $P^{-1}P$ is made up solely of products of some $l_i$ and some $\mathbf{r_i}$, and is the identity, there necessarily must be some $l_i$ and some $\mathbf{r_i}$ that yield a 1.

\subsection*{4.11 (iii)}

Consider the matrix 

\[A = \begin{bmatrix}
1 & -1\\
1 & 3
\end{bmatrix}\]

\begin{align*}
\lambda &= \rho = 2 \neq 0\\
\mathbf{\ell} &= \begin{bmatrix}
1\\
1
\end{bmatrix} \ \neq 0\\
\mathbf{r} &= \begin{bmatrix}
-1\\
1
\end{bmatrix} \neq 0 \\
\mathbf{\ell r} &= 0
\end{align*}

Note that the eigenvectors are distinct and non-zero, while the eigenvalues are the same and non-zero. Their product, however, is zero.


\subsection*{4.12}
Consider the matrix
\[ A = 
\begin{bmatrix}
    2 & 3 \\
    0 & -2
    
\end{bmatrix}\]
The eigenvalues of $A$ are 2 and -2 with eigenvectors
\[
\begin{bmatrix}
    1 \\
    0
    
\end{bmatrix}
\begin{bmatrix}
    -3 \\ 
    4
    
\end{bmatrix}
\]

However, there is no dominant eigenvalue, since the absolute value of these two is the same. The problem arises in that the power method fails to converge to one eigenvector and simply oscillates between two eigenvectors. 

\subsection*{4.13}

Since $T$ is an orthonormal operator on $V$, we know that there is some matrix $D$ and a matrix $P$ such that 
\begin{align*}
T = PDP^{-1}
\end{align*}
Now, let 
\begin{align*}
Q = PV^{-1}
\end{align*}
By theorem, since $P$ and $V^{-1}$ are orthogonal, their product, $Q$, is orthogonal. Furthermore, $P$ is an orthonormal eigenbasis for $T$. Now we have that
\begin{align*}
Q &= PV^{-1}\\
QV &= PV^{-1}V\\
QV &= P
\end{align*}
Therefore, $QV^{-1}$ is at least an orthogonal eigenbasis for $T$, and since we are trying to prove the existence of $Q$, we can divide each column of the eigenbasis by its length in order to normalize the eigenbasis, and $QV^{-1}$ is an orthonormal eigenbasis for $T$.

\subsection*{4.14}
Let A be Hermitian. Then we have that 
\[\langle x, Ax \rangle =  x^H, Ax = x^H, A^Hx = (x^HAx)^H = \bar{\langle x, Ax \rangle}\] 
As this is equal to itself, the elements thereof can only be real. The Rayleigh quotient's denominator is a norm, which is real, a real number over another real is real. 
Now let A be skew-Hermitian. Then we have that 
\[\langle x, Ax \rangle =  -x^H, Ax = -x^H, A^Hx = -(x^HAx)^H = -\bar{\langle x, Ax \rangle}\] 
This implies that the number at hand is pure imaginary. Again, the norm by which we divide the $\langle x, Ax \rangle$ is real, and an imaginary divided by a real is an imaginary.

\subsection*{4.15 (i)}
By proposition 3.7.12, If $V$ is a finite-dimensional inner product space, then we have
\begin{align*}
&(i). If ~S \in V,~ then~ S^{**} = S\\
&(ii). If ~S,T \in V, ~then (ST)^* = T^*S^*
\end{align*}
Therefore we have that 
\begin{align*}
(TT^*)^* = T^{**}T^* = TT^* \\
\end{align*}
So $TT^*$ is self-adjoint. We also have that
\begin{align*}
\langle \mathbf{w}, TT^* \mathbf{w} \rangle = \langle T^* \mathbf{w}, T^* \mathbf{w} \rangle \geq 0
\end{align*}


\subsection*{4.15 (ii)}
As $TT^*$ is normal, we have that $TT^* = UDU^{-1}$. Let $T = USU^{-1}$ where $S$ is the matrix $D$'s element-wise square root. Note that
\[ S^2 = U\sqrt{D}U^{-1}U\sqrt{D}U^{-1} = U\sqrt{D}^2U{-1} = UDU^{-1} = TT^*\]
    
Now, we have
\[SS = TT^*
S = TT^*S^{-1}
S^* = (TT^*S^{-1})^*
S^* = {S^{-1}}^*TT^*
S^*S^* = TT^* = SS
\]
So S is self-adjoint.

\subsection*{4.15 (iii)}

\begin{align*} 
(S^{-1}T)^*S^{-1}T &= T^*(S^{-1})^*S^{-1}T\\
&= T^*S^{-1}S^{-1}T\\
&= T^*(S^{-1})^{2}T\\
&= T^*(S^2)^{-1}T\\
&= T^*(TT^*)^{-1}T\\
&= T(T^*)^{-1}T^{-1}T\\
&= T(T)^{-1}T^{-1}T\\
&= I\\
\end{align*}

Since $(S^{-1}T)^*S^{-1}T$ yields the identity, we know that $(S^{-1}T)$ is orthonormal.\\

\subsection*{4.17(i)}
$A$ is normal with eigenvalues $\{\lambda_{1},\lambda_{2},...,\lambda_{n}\}$ and eigenvectors$\{x_{1}, x_{2},..., x_{n}\}$. We express $A$ as $A = UDU^{-1}$. $U$ is an orthonormal eigenbasis and so we have that $U^{H}=U^{-1}$ and that $U^{H}U = UU^{H}$. We know that $UU^{-1} = UU^{H} =  U^{H}U = I$. \\
Since $U$ is an orthogonal matrix, we know that any column of $U$ multiplied by any row of $U^H$ will yield a zero scalar unless they are the same eigenvector. If they are the same eigenvector, the product will be a one, resulting in $x_{i}x_{i}^{H}$. Concretely, this means $U^{H}U = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$, and thus $I = x_{1}x_{1}^{H} + x_{2}x_{2}^{H} + ... + x_{n}x_{n}^{H}$. \\ 

\subsection*{4.17 (ii)}


Using the same decomposition as in (i), we can express $A$ in the following manner:
\begin{align*}A &= U^HDU\\
& = \begin{bmatrix}
\mathbf{x_1, x_2, ..., x_n}\end{bmatrix}^{\mathbf{H}}
\begin{bmatrix}
{\lambda_1, \lambda_2, ..., \lambda_n}
\end{bmatrix}
\begin{bmatrix}
\mathbf{x_1, x_2, ..., x_n}\end{bmatrix}
\end{align*}
where the columns of $U$ are the columns $\{\mathbf{x_1, x_2, ..., x_n}\}$, the diagonal of $D$ consists of the eigenvalues $\{\lambda_1, \lambda_2, ..., \lambda_n\}$, and the rows of $U^H$ consist of $\{\mathbf{x_1^H, x_2^H, ..., x_n^H}\}$. If we multiply these matrices, we get 
\begin{align*} A &= \begin{bmatrix}
\lambda_1\mathbf{\bar{x}_{11}x_{11}} + \lambda_2\mathbf{\bar{x}_{21}x_{21}} +...+ \lambda_n\mathbf{\bar{x}_{n1}x_{n1}} & ... & 
\lambda_1\mathbf{\bar{x}_{1n}x_{11}} + \lambda_2\mathbf{\bar{x}_{2n}x_{21}} +...+\lambda_n\mathbf{\bar{x}_{nn}x_{n1}} \\
\vdots&\ddots&\vdots\\
\lambda_1\mathbf{\bar{x}_{11}x_{1n}} + \lambda_2\mathbf{\bar{x}_{21}x_{2n}} +...+\lambda_n\mathbf{\bar{x}_{n1}x_{nn}} & ... &
\lambda_1\mathbf{\bar{x}_{1n}x_{1n}} + \lambda_2\mathbf{\bar{x}_{2n}x_{2n}} +...+ \lambda_n\mathbf{\bar{x}_{nn}x_{nn}} 
\end{bmatrix}\\
\\
&= \lambda_1 \mathbf{x_1x_1^H} + \lambda_2 \mathbf{x_2x_2^H} + ... + \lambda_n \mathbf{x_nx_n^H}
\end{align*}

\subsection*{4.19}
$(\Rightarrow)$ Consider the following:
\begin{align}
A &= \begin{bmatrix}
B & C \\ 
C^H & D
\end{bmatrix}\\
P^HAP &= \begin{bmatrix}
B & BE + C \\
E^HB +C^H & E^HBE + C^HE + E^HC + D
\end{bmatrix}\\
&=\begin{bmatrix}
B & 0\\
0 & D - C^HB^{-1}C
\end{bmatrix}\\
&= \begin{bmatrix}
B&0\\
0 &0
\end{bmatrix}
+
\begin{bmatrix}
0&0\\
0 & D-C^HB^{-1}C
\end{bmatrix}
\end{align}
Since both matrices of (4) are positive definite, meaning that they have strictly positive eigenvalues. Therefore, their sum, call it matrix $B$, will be positive definite, and since $B$ is similar to $A$, $A$ is also positive definite.\\
$(\Leftarrow)$ Taking into account the facts from the forward part of the proof, we now have that $A$ is positive definite, so $B$ is positive definite, and the two matrices that make it up in (4) are therefore positive definite.


\subsection*{4.20}


Remember that $tr(CM) = tr(MC)$ for two square matrices M and C. Now let
\[A = PD_1P^H\\ 
B=RD_2R^H\] where $P$ and $R$ are orthonormal matrices and $D_1$ and $D_2$ are diagonal matrices. Let $d_{ij}$ and $\delta_{ij}$ be the $(i,j)$ entries of $D_1$ and $D_2$ respectively. Then 
\[tr(A)tr(B) = tr(UD_1U^H)tr(VD_2V^H) = tr(U^HUD_1)tr(V^HVD_2) = tr(D_1)tr(D_2) =\]
\[=\sum\limits_{i=1}^n d_{ii}\sum\limits_{i=1}^n \delta_{ii} \geq \sum\limits_{i=1}^{n}d_{ii}\delta_{ii} = tr(D_1D_2).\]

\[A = PD_1P^H = 
\begin{bmatrix}
u_{11} & u_{12}&\dots & u_{1n} \\
u_{21} & u_{22}&\dots & u_{2n}  \\
\vdots & \vdots &\ddots & \vdots \\
u_{n1} & u_{n2}&\dots & u_{nn}  \\
\end{bmatrix}
%
\begin{bmatrix}
d_{11} &              &\text{\huge0} &  \\
       & d_{22}       &              &  \\
       &              &\ddots        &  \\
       & \text{\huge0}&              & d_{nn}  \\
\end{bmatrix}
=
\begin{bmatrix}
u_{11}d_{11} & u_{12}d_{12}&\dots & u_{1n}d_{1n} \\
u_{21}d_{11} & u_{22}d_{12}&\dots & u_{2n}d_{1n}  \\
\vdots & \vdots &\ddots & \vdots \\
u_{n1}d_{11} & u_{n2}d_{12}&\dots & u_{nn}d_{1n}  \\
\end{bmatrix}
\]
\[U^H \\
=
\begin{bmatrix}
u_{11}d_{11} & u_{12}d_{12}&\dots & u_{1n}d_{1n} \\
u_{21}d_{11} & u_{22}d_{12}&\dots & u_{2n}d_{1n}  \\
\vdots & \vdots &\ddots & \vdots \\
u_{n1}d_{11} & u_{n2}d_{12}&\dots & u_{nn}d_{1n}  \\
\end{bmatrix}
%
\begin{bmatrix}
\overline{u_{11}} & \overline{u_{21}}&\dots  & \overline{u_{n1}} \\
\overline{u_{12}} & \overline{u_{22}}&\dots  & \overline{u_{n2}}  \\
\vdots            & \vdots           &\ddots & \vdots             \\
\overline{u_{1n}} & \overline{u_{2n}}&\dots  & \overline{u_{nn}}  \\
\end{bmatrix}
\]
So we have that\\
$a_{11} = d_{11}u_{11}\overline{u_{11}} + d_{22}u_{12}\overline{u_{12}} + d_{33}u_{13}\overline{u_{13}} + ... + d_{nn}u_{1n}\overline{u_{1n}}$\\

$a_{22} = d_{11}u_{21}\overline{u_{21}} + d_{22}u_{22}\overline{u_{22}} + d_{33}u_{23}\overline{u_{23}} + ... + d_{nn}u_{2n}\overline{u_{2n}}$\\

 $a_{ii} = d_{11}u_{i1}\overline{u_{i1}} + d_{22}u_{i2}\overline{u_{i2}} + d_{33}u_{i3}\overline{u_{i3}} + ... + d_{nn}u_{in}\overline{u_{in}}$\\

$a_{nn} = d_{11}u_{n1}\overline{u_{n1}} + d_{22}u_{n2}\overline{u_{n2}} + d_{33}u_{n3}\overline{u_{n3}} + ... + d_{nn}u_{nn}\overline{u_{nn}}$\\ 
 

So $tr(A) =$\\

$d_{11}( u_{11}\overline{u_{11}} +  u_{21}\overline{u_{21}} + ... + u_{i1}\overline{u_{i1}} + ...u_{n1}\overline{u_{n1}} )$\\
$+ d_{22}( u_{12}\overline{u_{12}} +  u_{22}\overline{u_{22}} + ... + u_{i2}\overline{u_{i2}} + ...u_{n2}\overline{u_{n2}} )$\\
$\vdots$\\
$+ d_{nn}( u_{1n}\overline{u_{1n}} +  u_{2n}\overline{u_{2n}} + ... + u_{in}\overline{u_{in}} + ...u_{nn}\overline{u_{nn}} )$\\

And since $U$ is orthonormal we have $UU^H = I$ and $U^HU = I$


Now concerning the Frobenius norm, we will consider positivity, scalar preservation, and the Triangle Inequality.
As for positivity, proposition 4.4.7 suggests that $AA^H$ is positive semi-definite. 
\[||A||_F = \sqrt{tr(AA^H)} = \sqrt{tr(AA^H)}\]
$I$ is also positive semi-definite. So by our proof of the first part of the proof, the trace of the product of two distinct positive semi-definite matrices is nonnegative. 

Scale Preservation:\\
\[||\alpha A||_F =\sqrt{tr(\alpha A \alpha A^H)} = \sqrt{  \alpha^2tr(AA^H)} = \sqrt{\alpha^2}\sqrt{tr(AA^H)} = |\alpha|||A||_F.\]

Triangle Inequality:\\
\[||A+B||_F = \sqrt{ tr( (A+B)(A+B)^H    )     } =\]
\[\sqrt{ tr( (A+B)(A^H+B^H)    )     } = \sqrt{ tr( AA^H + AB^H +BA^H+BB^H    )     }\]
\[=\sqrt{tr(AA^H) + tr(BB^H) + 2tr(AB^H)       }\]

So \[||A+B||^2 = tr(AA^H) + tr(BB^H) + 2tr(AB^H)\]

\[||A||_F + ||B||_F = \sqrt{tr(AA^H)} + \sqrt{tr(BB^H)}\]

\[(||A||_F + ||B||_F)^2 = ||A||_F^2 +2||A||_F||B||_F + ||B||_F^2 = tr(AA^H)+ 2\sqrt{tr(AA^H)tr(BB^H)} + tr(BB^H)\]


$tr(AA^H)tr(BB^H) = ||A||_F^2||B||_F^2$ and by Cauchy-Schwarz we have\\
$ ||A||_F^2||B||_F^2 \geq \langle A,B\rangle^2 =tr(AB^H)^2$\\













\subsection*{4.21 (i)}

$AA^H = \begin{bmatrix}
    3 & 0 & 6\\
    0 & 0 & 0\\
    6 & 0 & 12
\end{bmatrix}$ $A^HA=\begin{bmatrix}
    5&5&5&0\\
    5&5&5&0\\
    5&5&5&0\\
    0&0&0&0
\end{bmatrix}$\\


        $\lambda = 15, 0$\\
        15 has a algebraic multiplicity of 1, geometric multiplicity of 1, 0 has an algebraic multiplicity of 3, geometric multiplicity of 3.
        \[ \lambda(15)= \begin{bmatrix} 1\\ 1\\ 1\\ 0 \end{bmatrix} 
        \lambda(0)= \begin{bmatrix} 1\\0\\-1\\0\end{bmatrix}
        \lambda(0) = \begin{bmatrix} 0\\0\\0\\1 \end{bmatrix}
        \lambda(0) = \begin{bmatrix} 0\\1\\-1\\0 \end{bmatrix}\]\\

\subsection*{4.21 (ii)}


Singular values: $\sqrt{15}, 0$

\subsection*{4.21 (iii)}


        $U = \begin{bmatrix} 
        \frac{1}{\sqrt{3}} & 0 & \frac{2}{\sqrt{3}}\\
        0 & 1 & 0\\
        \frac{2}{ \sqrt{3}} & 0 & \frac{-1}{\sqrt{3}}
    \end{bmatrix}~
    \Sigma = \begin{bmatrix}
        \sqrt{15} &0&0&0\\
        0&0&0&0\\
        0&0&0&0
    \end{bmatrix}~
    V = \begin{bmatrix} 
        \frac{1}{\sqrt{3}} & \frac{1}{\sqrt{3}} & 0 & 0\\
        \frac{1}{\sqrt{3}} & 0 & \frac{1}{\sqrt{2}} & 0\\
        \frac{1}{\sqrt{3}} & \frac{-1}{\sqrt{2}} & \frac{-1}{\sqrt{2}} &0\\
        0&0&0&1
    \end{bmatrix}\\$

\subsection*{4.21 (iv)}

        $\mathcal{R}(A) = \begin{bmatrix} \frac{1}{\sqrt{3}}\\0\\
            \frac{2}{\sqrt{3}}\end{bmatrix}~ \mathcal{R}(A^H) = \begin{bmatrix}
            \frac{1}{\sqrt{3}}\\ \frac{1}{\sqrt{3}} \\ \frac{1}{\sqrt{3}}\\0
        \end{bmatrix}~  \mathcal{N}(A)= \begin{bmatrix} 0\\1\\0 \end{bmatrix}
        \begin{bmatrix} \frac{2}{\sqrt{3}}\\0\\ \frac{-1}{\sqrt{3}}
        \end{bmatrix}~ \mathcal{N}(A^H) = \begin{bmatrix} \frac{1}{\sqrt{3}} \\
            0 \\ \frac{-1}{\sqrt{2}} \\ 0 \end{bmatrix}
        \begin{bmatrix} 0\\ \frac{1}{\sqrt{2}}\\ \frac{-1}{\sqrt{2}}\\0
        \end{bmatrix}
        \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}$



\subsection*{4.22}



\[D=
\begin{bmatrix}
   0 &  1 & 0 \\
   0 &  0 & 2\\
   0 & 0 & 0\\
  \end{bmatrix}\]
SVD decomposition given by
\[
\begin{bmatrix}
   0 &  1 & 0 \\
   1 &  0 & 0\\
   0 & 0 & 1\\
  \end{bmatrix}
  \begin{bmatrix}
   2 &  0 & 0 \\
   0 &  1 & 0\\
   0 & 0 & 0\\
  \end{bmatrix}
  \begin{bmatrix}
   0 &  0 & 1 \\
   0 &  1& 0\\
   1 & 0 & 0\\
  \end{bmatrix}\]











\subsection*{4.23(i)}

    We have that $\frac{\langle Ax, Ax \rangle^{\frac{1}{2}}}{\|x\|}= \frac{\langle 
    A^HAx,x\rangle^\frac{1}{2}}{\|x\|} $
    The eigenbasis $\{v_1,v_2,\dots v_n\}$ corresponds to $A^HA$, and the corresponding eigenvalues will be the singular values. Therefore, 
    $x=\sum ^n_{i=1}a_iv_i$ because $\{v_1,v_2,\dots v_n\}$ forms an eigenbasis. Note that\\
    \begin{equation*}
        \frac{\langle A^HA\sum a_iv_i,x\rangle^{\frac{1}{2}}}{\|x\|} =  \frac{\langle 
            \sum \sigma_i^2 a_i v_i,x\rangle^{\frac{1}{2}}}{\|x\|} \leq \frac{\sigma \langle 
                \sum a_i v_i,x\rangle^{\frac{1}{2}}}{\|x\|}=\sigma_1
    \end{equation*}
    The vector we want to find will simply be the eigenvector that corresponds to $\sigma_1$

\subsection*{4.23 (ii)}

    A is invertible and square, so there exists some B such that ${A^{-1}}^HA^{-1} = B^HB$.
    Therefore, if we simply invert A we get $\Sigma^{-1}$ as the matrix containing the singular values of B, which are the singular values of A but inverted. The smallest singular value of $A$ is the largest singular value of $B$ since they are the inverted values, so the statement is proven.

\subsection*{4.23 (iii)}


We know that $\|A\|_{2} = \sigma_{1}$, so we have that $\|A\|_{2}^{2} = \sigma_{1}^{2}$ \\ \\
Now we have that:\\
   \[ A^{H} = (U \Sigma V^{H})^{H} = V \Sigma^{H} U^{H} \]
   As $U$ and $V$ are orthonormal matrices, $U^{H}$ and $V$ are also orthonormal matrices with the singular values of $\Sigma$ being the same as $\Sigma^{H}$ because the singular values of $\Sigma$ are already real and positive, therefore $\|A^{H}\|_{2}^{2} = \sigma_{1}^{2}$ \\ \\
   If this holds for $A^H$, it certainly holds for $A^T$.
   For the last part, we have that: \\

   \[ A^{H}A = (U\Sigma V^{H})^{H}(U\Sigma V^{H}) = V\Sigma^{H}U^{H} U\Sigma V^{H} = V\Sigma^{H}\Sigma V^{H} \]
   We know that $\Sigma$ is composed of the singular values of $A$, so $\Sigma^H \Sigma$ will yield a matrix with the squared singular values, and we have that $\|A^{H}A\|_{2} = \sigma_{1}^{2}$.
\subsection*{4.23 (iv)}
Since $A$ is an $mxn$ matrix, we can express it as 
\begin{align*}
    A = W\Sigma X^T
\end{align*}
where $W$ and $X$ are orthonormal matrices. Now, we have
\begin{align*}
    \|UAV\|_2 &= \|A\|_2\\
    &= \|UW\Sigma X^TV\|_2 
\end{align*}
Since we know the product of two orthonormal matrices is orthonormal, we know that the product of $U$ and $W$, call it $B$, and the product of $X^T$ and $V$, call it $C$, is orthonormal. Therefore, $A$ fulfills the criteria of the SVD and since we've already found that $\|A\|_2 = \sigma_1$ in exercise (i), $UAV$ has a singular decomposition with the same $\Sigma$ matrix as $A$, so $\|UAV\|_2 = \sigma_1$.


\subsection*{4.24}

First, since A is of rank r, let 
\[A = U\Sigma V^H \]

Note that

\[\sqrt{tr(AA^H)}=\sqrt{tr((U\Sigma V^H)(U\Sigma V^H)^H)}=\sqrt{tr(U\Sigma V^HV\Sigma^H U^H)}\]
\[\sqrt{tr(U\Sigma I\Sigma U^H)}=\sqrt{tr(U^H U\Sigma^2)}=\sqrt{tr(\Sigma^2)}=(\sigma_{1}^2+\sigma_{2}^2+...+\sigma _{r}^2)^{1/2}\]

\subsection*{4.26}

Let

\[A=U\Sigma V^H\]
Now we have that
\[|\det (A)|= |\det (U\Sigma V^H)|=|\det (U) \det (\Sigma) \det (V^H)|=1\cdot |\det \Sigma |\cdot 1\]
\[= |\sigma_1 \cdot \sigma_2 \cdot \sigma_3 \cdot ...\cdot \sigma _n|\]
\[= \sigma_1 \cdot \sigma_2 \cdot \sigma_3 \cdot ...\cdot \sigma _n\]
since all singular values are positive.



\subsection*{4.27}
Consider the matrix
\begin{align*}
    A = \begin{bmatrix}
    -1 & 0\\
    0 & -3
    \end{bmatrix}
\end{align*}
which has negative eigenvalues $-1$ and $-3$. Its determinant is non-zero and yet needs positive, real singular values. Therefore, the eigenvalues differ from the singular values and thus fulfills the criteria specified by the exercise.

\subsection*{4.28}

By flipping the SVD decomposition from 4.21, The result of the composition is 

\[A^{\dag}=\begin{bmatrix}
\frac{1}{15}&0&\frac{2}{15} \\
\frac{1}{15}&0&\frac{2}{15} \\
\frac{1}{15}&0&\frac{2}{15}\\
0&0&0
\end{bmatrix}\]

\[A^{\dag}A = 
\begin{bmatrix}
\frac{1}{3}&\frac{1}{3}&\frac{1}{3} &0\\
\frac{1}{3}&\frac{1}{3}&\frac{1}{3} &0\\
\frac{1}{3}&\frac{1}{3}&\frac{1}{3}&0\\
0&0&0&0
\end{bmatrix} A^HA =  
\begin{bmatrix}
5 & 5&5&0\\
5&5&5 & 0\\
5&5&5&0\\
0&0&0&0
\end {bmatrix}
\]

The two latter matrices are scalar multiples of each other.

\subsection*{4.34}



Note that 
\[\det(V_n)=\begin{vmatrix}
   1 &  x_0 & x_0^2&\hdots&x_0^n \\
   1 &  x_1& x_1^2&\hdots&x_1^n\\
   \vdots&\vdots&\vdots &\ddots&\vdots\\
   1 &  x_n& x_n^2&\hdots&x_n^n\\
   \end{vmatrix}\]
   Upon transposing the determinant,
\[=\begin{vmatrix}
   1 &  1& 1&\hdots&1 \\
   x_0 &  x_1& x_2&\hdots&x_n\\
   \vdots&\vdots&\vdots &\ddots&\vdots\\
   x_0^n &  x_1^n& x_2^n&\hdots&x_n^n\\
   \end{vmatrix}\]
   By row reduction we have
   \[=\begin{vmatrix}
   1 &  1& 1&\hdots&1 \\
   0 &  (x_1-x_0)& (x_2-x_0) &\hdots&(x_n-x_0)\\
   0&x_1(x_1-x_0)& x_2(x_2-x_0)&\hdots& x_n(x_n-x_0)\\
   \vdots&\vdots&\vdots &\ddots&\vdots\\
   0 &  x_1^{n-1}(x_1-x_0)& x_2^{n-1}(x_2-x_0)&\hdots&x_n^{n-1}(x_p-x_0)\\
   \end{vmatrix}\]
   \[=\begin{vmatrix}
   (x_1-x_0)& (x_2-x_0) &\hdots&(x_n-x_0)\\
   x_1(x_1-x_0)& x_2(x_2-x_0)&\hdots& x_n(x_n-x_0)\\
   \vdots&\vdots &\ddots&\vdots\\
  x_1^{n-1}(x_1-x_0)& x_2^{n-1}(x_2-x_0)&\hdots&x_n^{n-1}(x_p-x_0)\\
   \end{vmatrix}\]
   Factoring out terms from the transposed matrix we have that
   \[=\det(V_n)=(x_{1}-x_{0})(x_2-x_0)\hdots(x_n-x_0)
   \begin{vmatrix}
   1& 1 &\hdots&1\\
   x_1& x_2&\hdots& x_n\\
   \vdots&\vdots &\ddots&\vdots\\
  x_1^{n-1}& x_2^{n-1}&\hdots&x_n^{n-1}\\
   \end{vmatrix} \]
   Performing this same operation iteratively, we get
\[\det (V_n)=\prod _{i<j}(x_j-x_i)\]









\subsection*{4.35}
\[(AB)_{ij} = \Sigma_{k=1}^n a_{i,k}b_{kj} \geq 0\]
Since both parts of these are non-negative, this condition is fulfilled.
\[\Sigma_i(AB)_{ij} = \Sigma_j\Sigma_k a_{ik} b_{kj}\\
 = \Sigma_k\Sigma_j a_{ik} b_{kj}\\
 = \Sigma_k a_{ik} (\Sigma_j b_{kj})\\
 =  \Sigma_k a_{ik} = 1 
 \]
since the sum of the elements of A and B equal 1.
As for $\mathbf{x}$ we will solve

\[ 
Ax = x\\
\Sigma_k a_{ik}x_k = x_i\]
    
And since $\Sigma_k a_{ik} = 1$, we have $x = [ \frac{1}{n}, \dots, \frac{1}{n} ]^T$, and by the Gershgorin theorem we have that  
\[ |\lambda_i| \leq \Sigma_j a_{ij} \leq 1\]


\subsection*{4.36}

Assume that 3 has a geometric multiplicity of 1 and that -5 has a geometric multiplicity of 1. Its Jordan Form would be:

\[J = \begin{bmatrix}
   -5 & 1 & 0 & 0\\
   0 & -5 & 0 & 0\\
   0 & 0 & 3 & 1\\
   0 & 0 & 0 & 3\\
  \end{bmatrix}\]\\
Now assume that 3 has a geometric multiplicity of 2 and -5 has a geometric multiplicity of 1. Its Jordan Form would be:
\[J = \begin{bmatrix}
   -5 & 1 & 0 & 0\\
   0 & -5 & 0 & 0\\
   0 & 0 & 3 & 0\\
   0 & 0 & 0 & 3\\
  \end{bmatrix}\]
Now assume that 3 has a geometric multiplicity of 1 and -5 has a geometric multiplicity of 2. Its Jordan Form would be:
\[J =\begin{bmatrix}
   -5 & 0 & 0 & 0\\
   0 & -5 & 0 & 0\\
   0 & 0 & 3 & 1\\
   0 & 0 & 0 & 3\\
  \end{bmatrix}\]\\
Now assume that 3 has a geometric multiplicity of 2 and -5 has a geometric multiplicity of 2. Its Jordan Form would be:\\
\[J =\begin{bmatrix}
   -5 & 0 & 0 & 0\\
   0 & -5 & 0 & 0\\
   0 & 0 & 3 & 0\\
   0 & 0 & 0 & 3\\
  \end{bmatrix}\]\\

\subsection*{4.37}

We know that the matrix has an eigenvalue of $\alpha$ with an algebraic multiplicity of 3 and a geometric multiplicity of 1, yielding an eigenvector of 
\[ 
\begin{bmatrix}
    1\\
    0\\
    0
    
\end{bmatrix}\]

Jordan-form matrix, then, is 

\[ 
J = \begin{bmatrix}
    \alpha & 1 + \alpha & b + a\alpha\\
    0 & \alpha & b\alpha + 2c\\
    0 & 0 & c\alpha
    
\end{bmatrix}\]


\[ 
P = \begin{bmatrix}
    1 & 1 & 0\\
    0 & 1 & 1\\
    0 & 0 & \frac{1}{2} 
    
\end{bmatrix}\]

\end{document}
