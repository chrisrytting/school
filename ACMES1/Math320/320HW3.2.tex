\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{mathrsfs}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{listings}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}

\title{}
\author{Chris Rytting}
\maketitle

\subsection*{3.7}
We want to show that $p'$ is, in fact, a probability measure. To do so, we need to show that 
\begin{align*}
    &(i) ~ p'(F) = 1\\
    &(ii) ~ \text{Additivity: If} \{E_i\}_{i \in I} \subset \mathscr{F}' \text{ is a collection of pairwise-disjoint events, indexed by}\\&\text{ a countable set $I$, then} \\\\
    & \quad p' \Big( \cup_{i \in I} E_i  \Big) = \sum_{i \in I} p'(E_i)
\end{align*}

(i) is obvious, as noted in the remark, that $p'$ is the probability metric defined on $\mathscr{F}'$ such that $p'(F) =1$, since $\mathscr{F}'$ is the power set of $F$.
\\\\
As for (ii), we note that this condition was fulfilled for $\Omega$ when it was our sample space. Since $E' \subset E$, we know that the same condition will apply when $F$ is our new sample space. 

\subsection*{3.8}
We know, using DeMorgan's Law and definition 3.2.3, that
\[P \Big( \cup^{n}_{k=1} E_k \Big) = 1 - P \Big(  \Big( \cup^{n}_{k=1} E_k \Big)^c \Big)  = 1 - P  \Big( \cap^{n}_{k=1} E_k^c \Big)  = 1 - \Pi_{k=1}^nP(E_k^c) = 1 - \Pi_{i=1}^n (1-P(E_k))\]
Which is the desired result.

\subsection*{3.9}
Using Bayes formula, we have that
\[ .004 \text{ prevalence }\quad .95 \text{ sensitivity }\quad .95 \text{ specificity } \implies P(C|T^+) = .070896 \]
\[ .004 \text{ prevalence }\quad .95 \text{ sensitivity }\quad .90 \text{ specificity } \implies P(C|T^+) = .03675 \]
\[ .004 \text{ prevalence }\quad .95 \text{ sensitivity }\quad .999 \text{ specificity } \implies P(C|T^+) = .79233 \]
\[ .004 \text{ prevalence }\quad .90 \text{ sensitivity }\quad .95 \text{ specificity } \implies P(C|T^+) = .07116 \]
\[ .004 \text{ prevalence }\quad .999 \text{ sensitivity }\quad .95 \text{ specificity } \implies P(C|T^+) = .07063 \]
\[ .001 \text{ prevalence }\quad .95 \text{ sensitivity }\quad .95 \text{ specificity } \implies P(C|T^+) = .01866\]
\[ .05 \text{ prevalence }\quad .95 \text{ sensitivity }\quad .95 \text{ specificity } \implies P(C|T^+) = .5 \]

\subsection*{3.10}
Where $R$ implies that the witness was right, and $W$ implies that the witness was wrong. Then we have that
\begin{align*}
    P(Blue) &= P(R|Blue) - P(W|Red)\\
    &= \frac{P(Blue|R)P(R)}{P(Blue|W)P(W) + P(Blue|R)P(R)} - \frac{P(Red|W)P(W)}{P(Red|W)P(W) + P(Red|R)P(R)}\\
    &= \frac{(.1)(.8)}{(.1)(.2) + (.1)(.8)} -  \frac{(.9)(.2)}{(.9)(.2) + (.9)(.1)} = .1333333
\end{align*}

\subsection*{3.11}

\[\frac{1\left( \frac{1}{250000000} \right)}{ 1 \left( 1 \frac{1}{250000000} \right) + \frac{1}{3000000}(1- \frac{1}{250000000})} = .01186\]





\subsection*{3.12}
Let the doors be denoted $D_1, D_2, D_3$. The probability that the car is behind any one of these doors is $\frac{1}{3}$, and the probability that a goat is behind any one of these doors is $\frac{2}{3}$. Say we pick $D_1$. This means that there is a $\frac{1}{3}$ chance that the door picked has the car, meaning that there is a $\frac{2}{3}$ chance that one of the other doors has the car. Once Monty opens one of the other doors, say $D_2$, the probability that the car lies behind $D_3$ is now $\frac{2}{3}$ because $D_2$ had a goat, and so the $\frac{2}{3}$ probability concentrates into $D_3$.\\\\
Ultimately, if you switch doors at this point, you double your odds of getting the car.
\\
By the same logic, but with doors $D_1, D_2,\cdots, D_{10}$, the likelihood that any given door has the car is initially $\frac{1}{10}$, meaning that the likelihood that one of the other doors has the car is $\frac{9}{10}$. So if Monty opens 8 of the other doors, your probability is now $\frac{9}{10}$ (if you switch doors) that the new door you pick will have the car behind it, increasing your odds by 9.



\end{document}
