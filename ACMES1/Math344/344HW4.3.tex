\documentclass[letterpaper,12pt]{article}

\usepackage{threeparttable}
\usepackage{geometry}
\geometry{letterpaper,tmargin=1in,bmargin=1in,lmargin=1.25in,rmargin=1.25in}
\usepackage[format=hang,font=normalsize,labelfont=bf]{caption}
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{array}
\usepackage{delarray}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{lscape}
\usepackage{natbib}
\usepackage{setspace}
\usepackage{float,color}
\usepackage[pdftex]{graphicx}
\usepackage{pdfsync}
\usepackage{verbatim}
\usepackage{placeins}
\usepackage{geometry}
\usepackage{pdflscape}
\synctex=1
\usepackage{hyperref}
\hypersetup{colorlinks,linkcolor=red,urlcolor=blue,citecolor=red}
\usepackage{bm}


\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}{Definition} % Number definitions on their own
\newtheorem{derivation}{Derivation} % Number derivations on their own
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}{Proposition} % Number propositions on their own
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\bibliographystyle{aer}
\newcommand\ve{\varepsilon}
\renewcommand\theenumi{\roman{enumi}}
\newcommand\norm[1]{\left\lVert#1\right\rVert}

\begin{document}
\title{Math 344 Homework 4.3}
\author{Chris Rytting}
\maketitle

\subsection*{4.12}
Given the characteristic polynomial $\lambda^2- 1.4 \lambda + .4$, we have that our eigenvalues are $\lambda_1 = 1, \lambda_2 = .4$, yielding
\[ 
\begin{bmatrix}
   .8 -1 & .4 \\  
   .2 & .6 - 1 \\  
\end{bmatrix}
=
\begin{bmatrix}
   -.2 & .4 \\  
   .2 & -.4  \\  
\end{bmatrix}
=
\begin{bmatrix}
   -.2 & .4 \\  
   0 & 0  \\  
\end{bmatrix}
\]
Whose null space is 
\[ 
\begin{bmatrix}
    2 \\ 1
\end{bmatrix}
\] 
As for the second eigenvalue, we have that
\[ 
\begin{bmatrix}
   .8 -.4 & .4 \\  
   .2 & .6 - .4 \\  
\end{bmatrix}
=
\begin{bmatrix}
   .4 & .4 \\  
   .2 & .2 \\  
\end{bmatrix}
=
\begin{bmatrix}
   .4 & .4 \\  
   0 & 0\\  
\end{bmatrix}
\]
whose null space is
\[ 
\begin{bmatrix}
    1 \\ -1
\end{bmatrix}
\]
And we have that 
\[
P = 
\begin{bmatrix}
    2 & 1 \\
    1 & -1
\end{bmatrix}
D = 
\begin{bmatrix}
    1 & 0 \\
    0 & .4
\end{bmatrix}
P^{-1} = 
\begin{bmatrix}
    1/3 & 1/3 \\
    1/3 & -2/3
\end{bmatrix}
\]
\subsection*{4.13}
From Exercise 2 we know that\\
\[
\smallskip\\
D = \begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0
\end{bmatrix}\\
\smallskip\\\]
Thus the Charcteristic equation is given by
\[ \begin{vmatrix}
-\lambda & 1 & 0\\
0 & -\lambda & 2\\
0 & 0 & -\lambda
\end{vmatrix} \Rightarrow \lambda^3 = 0 \Rightarrow \lambda = 0\]
The eigen vectors are then given by the nullspace of \\
\smallskip\\ 
$\begin{bmatrix} 
    0 & 1 & 0\\
    0 & 0 & 2\\
    0 & 0 & 0
\end{bmatrix}$  
$\Rightarrow$ 
$\begin{bmatrix} 
    -1 & 0 & 0\\
    0 & 1 & 0\\
    0 & 0 & 2
\end{bmatrix}$
$\Rightarrow$
$\begin{bmatrix}
1\\
0\\
0
\end{bmatrix}$\\
\smallskip\\
Because the number of eigenvectors is less than the rank of D, we know that 
it is not semi-simple.

\subsection*{4.14 (i)}
        Let
\begin{align*}
    B = \lim_{k\to \infty}  P^{-1}(A)^kP & = \begin{bmatrix}\frac{1}{3} & \frac{1}{3} \\
                    \frac{1}{3} & \frac{-2}{3} \\
    \end{bmatrix}
    \begin{bmatrix}1 & 0 \\
                    0 & .4 \\
    \end{bmatrix}^k
    \begin{bmatrix}2 & 1 \\
                    1 & -1 \\
    \end{bmatrix}\\
    & = \begin{bmatrix}\frac{1}{3} & \frac{1}{3} \\
                    \frac{1}{3} & \frac{-2}{3} \\
    \end{bmatrix}
    \begin{bmatrix}1 & 0 \\
                    0 & 0 \\
    \end{bmatrix}
    \begin{bmatrix}2 & 1 \\
                    1 & -1 \\
    \end{bmatrix}\\
    & = \begin{bmatrix}\frac{2}{3} & \frac{2}{3} \\
                    \frac{1}{3} & \frac{1}{3} \\
    \end{bmatrix}
\end{align*}
and let
\begin{align*}
    C = \begin{bmatrix}0 & 0 \\
                    0 & 1 \\
    \end{bmatrix}\\
    E = D-C = \begin{bmatrix}.4 & 0 \\
                     0&  0\\
    \end{bmatrix}\\
    \text{Note, }\quad \quad &\\
    PE^xP^{-1} = \begin{bmatrix}\frac{2}{3}(.4^x) & 0 \\
                    \frac{2}{3}(.4)^x & 0 \\
    \end{bmatrix}
\end{align*}
Now, it should be clear that $\|A^k - B\|_1 = \frac{4}{3}(.4)^k < \epsilon \quad \epsilon > 0$, and we have that $\exists N >0$ such that 
\[\|A^k - B\|_1 < \epsilon \quad k > N\]
        \subsection*{4.14 (ii)}
        
    As for the infinity norm, we have that 
        \begin{align*}
            \|A^k - B\|_{\infty} & = \|P(D^k - C)P^{-1}\|\\
            & = \|PE^kP^{-1}\|_{\infty} \\
            & = \frac{2}{3}(.4)^k
        \end{align*}
        And we see that for $\epsilon > 0 \quad \exists N >0$, such that   
        \[ \frac{2}{3}(.4)^k < \epsilon \quad k > N\]
        As for the Frobenius norm, 
        \begin{align*}
            \|A^k - B\|_{F} & = \|P(D^k - C)P^{-1}_F\|\\
            & = \|PE^kP^{-1}\|_{F} \\
            & = \frac{2}{3}.4^k
        \end{align*}
        And we see that for $\epsilon > 0 \quad \exists N >0$, such that   
        \[ \frac{2}{3}(.4)^k < \epsilon \quad k > N\]
        The choice of norm is irrelevant.
    \subsection*{4.14 (iii)}
    
    
        By proposition 4.3.11, we have that the expression
        \begin{align*}
            p(A) = 3 + 5A + A^3
        \end{align*}
        is equivalent to 
        \[p(\lambda) = 3 + 5\lambda + \lambda^3\]
        yielding 
        \[3 + 5\cdot 1 + 1 = 9\quad 3 + .4\cdot5 + .4^3 = 2.064\]

\subsection*{4.15}
We know that 
\[Ax = \lambda_i x \forall i\]
Now, consider that
\[f(\lambda_i)x = a_0\lambda_i^0 + a_1\lambda_i^1 + \cdots +a_n\lambda_i^n \] 
and that
\[f(A)x = a_0A^0 + a_1A^1 + \cdots +a_nA^n \] 
Now, by proposition 4.3.9, we know that 
\[ \lambda_i x = A x \implies \lambda_i^k x = A^k x \]
and we have that
\[f(\lambda_i)x = a_0\lambda_i^0 + a_1\lambda_i^1 + \cdots +a_n\lambda_i^n 
=
 a_0A^0 + a_1A^1 + \cdots +a_nA^n 
\] 
which is the desired result, that $f(A)x = f(\lambda_i)x$.

\subsection*{4.16}
Let $A \in M_n(\mathbb{F}$, where $p(x)$ be characteristic polynomial.\\\\
Note that
\[A = PDP^{-1}\]
where $D$ is a diagonal matrix. Now we have that 
{\tiny
\begin{align*}
p(x) &= a_0A^0 + a_1A^1 +\cdots +a_nA^n \\
&= a_0(PDP^{-1})^0 + a_1(PDP^{-1})^1 +\cdots +a_n(PDP^{-1})^n \\
&= P(a_0D^0 + a_1D^1 +\cdots +a_nD^n)P^{-1} \\
&= P \left(  
\begin{bmatrix}
a_0 & 0 & \cdots  &  0 \\
0 & a_0 & \cdots  &  0 \\
\vdots & \vdots & \ddots &  \vdots \\
0 & 0 & \cdots  &  a_0 \\
\end{bmatrix}
+
\begin{bmatrix}
a_1\lambda_1 & 0 & \cdots  &  0 \\
0 & a_1\lambda_2 & \cdots  &  0 \\
\vdots & \vdots & \ddots &  \vdots \\
0 & 0 & \cdots  &  a_1\lambda_n \\
\end{bmatrix}
+
\begin{bmatrix}
a_2\lambda_1^2 & 0 & \cdots  &  0 \\
0 & a_2\lambda_2^2 & \cdots  &  0 \\
\vdots & \vdots & \ddots &  \vdots \\
0 & 0 & \cdots  &  a_2\lambda_n^2 \\
\end{bmatrix}
+
\cdots
+
\begin{bmatrix}
a_n\lambda_1^n & 0 & \cdots  &  0 \\
0 & a_n\lambda_2^n & \cdots  &  0 \\
\vdots & \vdots & \ddots &  \vdots \\
0 & 0 & \cdots  &  a_n\lambda_n^n \\
\end{bmatrix}
\right)P^{-1} \\
&= 
P \left( 
\begin{bmatrix}
a_0 + a_1\lambda_1 + a_2 \lambda_1^2 + \cdots + a_n \lambda_1^n & 0 & \cdots & 0 \\
0 & a_0 + a_1\lambda_2 + a_2 \lambda_2^2 + \cdots + a_n \lambda_2^n  & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0  & \cdots & a_0 + a_1\lambda_2 + a_2 \lambda_2^2 + \cdots + a_n \lambda_2^n \\
\end{bmatrix}
\right)P^{-1} \\
&= P \left( 
\begin{bmatrix}
    0  & 0 & \cdots & 0 \\
    0  & 0 & \cdots & 0 \\
    \vdots  & \vdots & \ddots & \vdots \\
    0  & 0 & \cdots & 0 \\
\end{bmatrix}\right)P^{-1}\\
&= 0
\end{align*}
}
Which is the desired result.

\subsection*{Exercise 4.17 (i)}

Note that \\
\[ \ell A = \lambda \ell \]
Therefore,
\[ \ell A r = \lambda \ell r \]

\[ \ell (A r) = \lambda \ell r \]

\[ \ell \rho r = \lambda \ell r \]

\[ \rho \ell r = \lambda \ell r \]
 
Since $\rho \neq \lambda$, the only way for this equality to hold is if $\ell r = 0$.



\subsection*{Exercise 4.17 (ii)}

By Remark 4.3.18, if a matrix is semisimple, it has a basis of right eigenvectors $\{\mathbf{p_1,...,p_n}\}$, and those right eigenvectors form the columns of a matrix $P$ which diagonalizes $A$. Furthermore, for each $i$ the $i$th row $\mathbf{q_i}$ of $P^{-1}$ is a left eigenvector of $A$ with eigenvalue $d_i$. Therefore, if we diagonalize the matrix $A$ and multiply $P^{-1}$ by $P$, we get the $I$, and we see that the rows $\mathbf{q_i}$ of $P^{-1}$ multiplied by the columns $\mathbf{p_i}$ of $P$ equals one at every entry of the identity's diagonal.

\[P^{-1}     P = I \]

\subsection*{Exercise 4.17 (iii)}

Consider the matrix 

\[A = \begin{bmatrix}
1 & -1\\
1 & 3
\end{bmatrix}\]

\begin{align*}
\lambda &= \rho = 2 \neq 0\\
\mathbf{\ell} &= \begin{bmatrix}
1\\
1
\end{bmatrix} \ \neq 0\\
\mathbf{r} &= \begin{bmatrix}
-1\\
1
\end{bmatrix} \neq 0 \\
\mathbf{\ell r} &= 0
\end{align*}

Note that the eigenvectors are distinct and non-zero, while the eigenvalues are the same and non-zero. Their product, however, is zero.


\subsection*{4.18}


Consider the matrix
\[A=\begin{bmatrix}
2 & 3\\
0&-2
\end {bmatrix}\]
Since the matrix is uppertriangular, the eigenvalues are  $\lambda_1=2$ and $\lambda_2=-2$, yielding eigenvectors 
\[\begin{bmatrix}
1\\
0
\end {bmatrix} \text{ and }
\begin{bmatrix}
-3\\
4\\
\end {bmatrix}\]
Upon performing the power method, we have
\[\textbf{x}_1=A\textbf{x}_0=
\begin{bmatrix}
2 & 3\\
0&-2
\end{bmatrix}
\begin{bmatrix}
1\\
1
\end{bmatrix}=
\begin{bmatrix}
5\\
-2
\end{bmatrix}
\]
\[\textbf{x}_2=A\textbf{x}_1=
\begin{bmatrix}
2 & 3\\
0&-2
\end{bmatrix}
\begin{bmatrix}
5\\
-2
\end{bmatrix}=
\begin{bmatrix}
4\\
4
\end{bmatrix}=4
\begin{bmatrix}
1\\
1
\end{bmatrix}
\]
\[\textbf{x}_3=A\textbf{x}_2=
\begin{bmatrix}
2 & 3\\
0&-2
\end{bmatrix}
\begin{bmatrix}
4\\
4
\end{bmatrix}=
\begin{bmatrix}
20\\
-8
\end{bmatrix}=4
\begin{bmatrix}
5\\
-2
\end{bmatrix}
\]
\[\textbf{x}_4=A\textbf{x}_3=
\begin{bmatrix}
2 & 3\\
0&-2
\end{bmatrix}
\begin{bmatrix}
20\\
-8
\end{bmatrix}=
\begin{bmatrix}
16\\
16
\end{bmatrix}=16
\begin{bmatrix}
1\\
1
\end{bmatrix}
\]

\[\vdots\]

Clearly, the method is oscillating between two distinct eigenvectors. We attribute this to separate dominant eigenvalues.\\\\
Now consider a matrix with two linearly independent eigenvectors but a dominant eigenvector

\[\begin{bmatrix}
5 & 0\\
0&5
\end {bmatrix}\]
Which has only one eigenvalue 5, yields two eigenvectors:
\[\begin{bmatrix}
1\\
0
\end {bmatrix} \text{ and }
\begin{bmatrix}
0\\
1
\end {bmatrix}\]
Starting with the initial guess from before, we get:

\[\textbf{x}_1=A\textbf{x}_0=
\begin{bmatrix}
5 & 0\\
0&5
\end{bmatrix}
\begin{bmatrix}
1\\
1
\end{bmatrix}=
\begin{bmatrix}
5\\
5
\end{bmatrix}
\]
\[\textbf{x}_2=A\textbf{x}_1=
\begin{bmatrix}
5 & 0\\
0&5
\end{bmatrix}
\begin{bmatrix}
5\\
5
\end{bmatrix}=25
\begin{bmatrix}
1\\
1
\end{bmatrix}
\]

\[\vdots\]

which converges to the wrong eigenvector.\\\\
We find that the power method requires a dominant eigenvalue with a geometric multiplicity of just 1.

































\end{document}

